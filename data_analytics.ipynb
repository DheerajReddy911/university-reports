{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJujPG8EvKoX5Rb3sImTQJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DheerajReddy911/university-reports/blob/main/data_analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4r8es7GGOhDg"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "-o5FrHX9Pfig"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZWlDXIePwTJ",
        "outputId": "a8813abd-27dc-4479-fab5-e65f03d443ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "synonyms = set()\n",
        "for syn in wordnet.synsets('car'):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())"
      ],
      "metadata": {
        "id": "Em1X7KFHP7AT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"synonyms for car are: \")\n",
        "for word in sorted(synonyms):\n",
        "    print(word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fk62gMltQX2s",
        "outputId": "c50794f8-b505-429f-869a-207ceb6e3533"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "synonyms for car are: \n",
            "auto\n",
            "automobile\n",
            "cable_car\n",
            "car\n",
            "elevator_car\n",
            "gondola\n",
            "machine\n",
            "motorcar\n",
            "railcar\n",
            "railroad_car\n",
            "railway_car\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "metadata": {
        "id": "bL86pXZVUf8W"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bep7xdkwUv4P",
        "outputId": "8af9aea5-3b85-4beb-d9b0-784d8a755820"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synonyms(word):\n",
        "    return list(set(lemma.name().replace('_', ' ')\n",
        "                for syn in wordnet.synsets(word)\n",
        "                for lemma in syn.lemmas()\n",
        "                if lemma.name().lower() != word.lower()))\n",
        "\n",
        "def synonym_replacement(sentence, n=2):\n",
        "    words = word_tokenize(sentence)\n",
        "    candidates = [w for w in words if get_synonyms(w)]\n",
        "    random.shuffle(candidates)\n",
        "    for i in range(min(n, len(candidates))):\n",
        "        word = candidates[i]\n",
        "        synonyms = get_synonyms(word)\n",
        "        if synonyms:\n",
        "            words[words.index(word)] = random.choice(synonyms)\n",
        "    return ' '.join(words)\n",
        "print(synonym_replacement(\"The quick brown fox jumps \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQSMsNxaVORG",
        "outputId": "c3d031bc-a5d7-474f-fde6-96f52bdafe91"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prompt brown fuddle jumps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import random\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def replace_entities(text):\n",
        "    doc = nlp(text)\n",
        "    new_text = text\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in replacements:\n",
        "            new = random.choice(replacements[ent.label_])\n",
        "            new_text = new_text.replace(ent.text, new)\n",
        "    return new_text\n",
        "\n",
        "\n",
        "sentence = \"Dheeraj was born in india.\"\n",
        "print(replace_entities(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0_phpdtXFNC",
        "outputId": "b2503243-5135-4231-d3d2-df721975619e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ram was born in Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def char_tokenize(text):\n",
        "    return list(text)\n",
        "sentence = \"Hello\"\n",
        "tokens = char_tokenize(sentence)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4sNDp57rJgC",
        "outputId": "46262352-5b5e-4962-a89d-e39c66fee7f1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['H', 'e', 'l', 'l', 'o']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "def char_level_tokenize_nltk(text):\n",
        "\n",
        "    tokenizer = RegexpTokenizer(r'.')\n",
        "    return tokenizer.tokenize(text)\n",
        "print(char_level_tokenize_nltk(\"hello\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBYTICmpwEaM",
        "outputId": "fe38a160-8e10-4e7b-e87d-b4630b64c023"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['h', 'e', 'l', 'l', 'o']\n"
          ]
        }
      ]
    }
  ]
}